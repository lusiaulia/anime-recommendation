# -*- coding: utf-8 -*-
"""Submission_Proyek_Kedua_ML_Terapan.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oqiPk5rsQAFIY85O7fL1ijDib5mjywh7

# Anime Recommendation System
- **Sumber Data:** https://www.kaggle.com/datasets/CooperUnion/anime-recommendations-database

##  Load Data
"""

# Commented out IPython magic to ensure Python compatibility.
#mengimpor library yang diperlukan
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
# %matplotlib inline
import seaborn as sns
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import TfidfVectorizer
import operator

# Clone dataset dari github
!git clone https://github.com/lusiaulia/anime-recommendation.git
# mendefinisikan dataframe
anime_df = pd.read_csv("/content/anime-recommendation/anime.csv")
rate_df = pd.read_csv("/content/anime-recommendation/rating.csv")
anime_df.head()

rate_df.head()

"""## Exploratory Data Analysis

Variabel-variabel dari anime dan rating dataset adalah sebagai berikut :

1. Variabel anime.csv yang selanjutnya disebut anime_df
<ul> <li>anime_id : id anime dari myanimelist.</li>
    <li>name : judul anime</li>
    <li>genre : genre anime</li>
    <li>type : movie, TV, OVA, dll.</li>
    <li>episodes : banyak episode (1 jika movie).</li>
    <li>rating : rata-rata rating dari anime (skala 1-10)</li>
    <li>members : jumlah member dalam grup anime</li>
</ul>

2. Variabel rating.csv yang selanjutnya disebut rate_df
<ul> <li>user_id : id user</li>
    <li>anime_id : anime yang dinilai/rate oleh user</li>
    <li>rating : rating skala 1-10 yang diberi oleh user (rating -1 jika sudah menonton tapi belum memberi rating)</li>
</ul>
"""

#melihat informasi jumlah dan jenis data
anime_df.info()

rate_df.info()

"""Berdasarkan output di atas, dapat diketahui bahwa rate_df memiliki 7813737 data entri, sementara anime_df memiliki 12294 entri."""

#melihat statistik deskriptif dari data
anime_df.describe()

rate_df.describe()

print('Jumlah animeID: ', len(anime_df.anime_id.unique()))
print('Jumlah animeID: ', len(rate_df.anime_id.unique()))

"""Dari data, diketahui bahwa rating berada di rentang 1-10, jumlah list anime ID pada anime_df ada 12294 list dan pada rate_df terdapat 11200 list anime ID. Selanjutnya akan dicari tahu apakah terdapat data kosong, duplikat pada dataset. Juga upaya untuk lebih merapihkan data sebelum dimodelkan.

## Data Preprocessing & Data Prep

Sudah disebutkan sebelumnya bahwa salah satu variabel pada rate_df yaitu rating terdapat 1 nilai yaitu '-1' yang berati user belum memberi penilaian dan sudah menonton. Agar tidak mempengaruhi hasil prediksi rating maka akan dihapus saja.
"""

rate_df = rate_df[rate_df['rating'] != -1]
rate_df

print('Jumlah animeID: ', len(rate_df.anime_id.unique()))

len(rate_df)

"""Saat ini dengan rate_df sejumlah 6337241 dan anime pada rate_df sejumlah 9927 anime yang berbeda (anime_id)

### Mengatasi null value
"""

#mengecek apakah ada data kosong
for column, i in anime_df.isnull().sum().items():
    if i > 0:
        print(f"{column}: {i/len(anime_df)*100:.2f}%")

#mengecek apakah ada data kosong
for column, i in rate_df.isnull().sum().items():
    if i > 0:
        print(f"{column}: {i/len(rate_df)*100:.2f}%")

"""Terdapat sejumlah kurang dari 2% data kosong dari anime_df, sehingga akan dilakukan penghapusan data kosong.Sementara pada rate_df tidak terdapat data kosong."""

anime_df = anime_df.dropna(axis=0).reset_index(drop=True)
anime_df.isnull().sum()

anime_df.info()

"""Setelah dilakukan penghapusan data kosong, saat ini terdapat sejumlah 12017 data entri pada anime_df. Selanjutnya mari dicek untuk data duplikat.

### Mengatasi data duplikat
"""

print(anime_df[anime_df.duplicated()].shape[0])
print(rate_df[rate_df.duplicated()].shape[0])

"""Tidak terdapat duplikat data pada data anime_df dan ada 1 data duplikat pada rate_df, akan dilakukan penghapusan data yang terduplikasi."""

rate_df.drop_duplicates(keep='first',inplace=True)
print(rate_df[rate_df.duplicated()].shape[0])

"""terlihat sudah tidak terdapat data duplikat. Kemudian dilakukan penghapusan duplikat juga pada pengguna dengan anime yang sama (asumsi 1 pengguna memberi 1 rating pada 1 anime yang ditonton)"""

rate_df = rate_df.drop_duplicates(['user_id', 'anime_id'])
len(rate_df)

"""Yang sebelumnya berjumlah 6337241 saat ini menjadi 6337234 data pada rate_df

### Visualisasi Data
"""

most_an = anime_df.sort_values(["members"],ascending=False)

plt.subplots(figsize=(15,10))
p = sns.barplot(x=most_an["members"], y=most_an["name"][:10])
p.axes.set_title("\nAnime dengan member komunitas terbanyak\n")
plt.xlabel("Total Member")
plt.ylabel("\nAnime Name")
for container in p.containers:
    p.bar_label(container,label_type = "center", color = "black")

sns.despine(left=True, bottom=True)
plt.show()

"""Terlihat jumlah dari member komunitas tiap anime berbeda-beda. Kemudian coba kita lihat jenis data pada kolom type pada anime_df"""

anime_df['type'].unique()

"""Ternyata tipe anime terbanyak yaitu yang muncul di televisi (TV),OVA, Movie, Spesial, ONA, dan Music. Secara visual terlihat di bawah ini"""

type_a = anime_df.sort_values(["type"],ascending=False)
def pie_chart(df, col, title):
    labels = [i for i in df[col].value_counts().index]
    df[col].value_counts().plot.pie(figsize=(8,8),
                                                 pctdistance=0.4,
                                                 startangle=90,
                                                 textprops={"color": "white"},
                                                 wedgeprops={"linewidth":0})
#                                    colors=['sandybrown','chocolate','brown','saddlebrown','black'])
    plt.title(title, fontsize=20, pad=5)
    box = plt.gca().get_position()
    plt.gca().set_position([box.x0, box.y0, box.width * 0.8, box.height])
    plt.legend(loc="center left", bbox_to_anchor=(1, 0.5), prop={"size":12,
                                                                 "weight":"ultralight"})
    plt.show();

pie_chart(type_a, "type", "Type anime terbanyak dari data")

type_an = ['Movie', 'TV', 'OVA', 'Special', 'Music', 'ONA']
counts={}
for i in type_an:
    counts[i] = (((anime_df['type'] == i).sum())/len(anime_df.type)*100)

print(counts)

"""Jumlah terbanyak yaitu anime yang tayang di TV (biasanya berepisode), disusul dengan OVA, Movie, Spesial, ONA dan Music."""

anime_df.rating.hist()

"""Dari histogram terlihat bahwa sebaran rating berada di rentang 2-9 dan paling banyak di kisaran rating 7

## Modeling & Evaluate Model
### User-Based Collaborative FilteringÂ¶
#### TF-IDF Vectorizer
"""

# Inisialisasi TfidfVectorizer
tf = TfidfVectorizer()
tfidf_matrix = tf.fit_transform(anime_df['name'])

# Melihat ukuran matrix tfidf
tfidf_matrix.shape

"""Perhatikanlah, matriks yang kita miliki berukuran (12017, 11951).

Untuk menghasilkan vektor tf-idf dalam bentuk matriks, kita menggunakan fungsi todense(). Jalankan kode berikut.
"""

# Mengubah vektor tf-idf dalam bentuk matriks dengan fungsi todense()
tfidf_matrix.todense()

# Membuat dataframe untuk melihat tf-idf matrix
pd.DataFrame(
    tfidf_matrix.todense(),
    columns=tf.get_feature_names_out(),
    index=anime_df.name
).sample(11952, axis=1).sample(10, axis=0)

"""### Cosine Similarity"""

# Menghitung cosine similarity pada matrix tf-idf
cosine_sim = cosine_similarity(tfidf_matrix)
cosine_sim

# Membuat dataframe dari variabel cosine_sim dengan baris dan kolom berupa id anime
cosine_sim_df = pd.DataFrame(cosine_sim, index=anime_df['name'], columns=anime_df['name'])
print('Shape:', cosine_sim_df.shape)

# Melihat similarity matrix pada setiap resto
cosine_sim_df.sample(5, axis=1).sample(10, axis=0)

"""### Mendapat Rekomendasi"""

def anime_recommendations(name, similarity_data=cosine_sim_df, items=anime_df[['name', 'anime_id','genre']], k=5):
    index = similarity_data.loc[:,name].to_numpy().argpartition(
        range(-1, -k, -1))

    closest = similarity_data.columns[index[-1:-(k+2):-1]]
    closest = closest.drop(name, errors='ignore')

    return pd.DataFrame(closest).merge(items).head(k)

anime_df[anime_df.name.eq('Shingeki no Kyojin')]

# Mendapatkan rekomendasi anime yang mirip dengan Shingeki no Kyojin
anime_recommendations('Shingeki no Kyojin')

"""### Model RecommederNet"""

import tensorflow as tf
from tensorflow import keras
from sklearn.model_selection import train_test_split
from tensorflow.keras import layers
from tensorflow.keras.layers import Input, Dense
from tensorflow.keras.models import Model

class RecommenderNet(tf.keras.Model):
    def __init__(self, num_users, num_anime, embedding_size, **kwargs):
        super(RecommenderNet, self).__init__(**kwargs)
        self.num_users = num_users
        self.num_anime = num_anime
        self.embedding_size = embedding_size

        # Layer embedding untuk pengguna
        self.user_embedding = layers.Embedding(
            num_users,
            embedding_size,
            embeddings_initializer='he_normal',
            embeddings_regularizer=keras.regularizers.l2(1e-6)
        )
        self.user_bias = layers.Embedding(num_users, 1)

        # Layer embedding untuk anime
        self.anime_embedding = layers.Embedding(
            num_anime,
            embedding_size,
            embeddings_initializer='he_normal',
            embeddings_regularizer=keras.regularizers.l2(1e-6)
        )
        self.anime_bias = layers.Embedding(num_anime, 1)

    def call(self, inputs):
        user_vector = self.user_embedding(inputs[:, 0])
        user_bias = self.user_bias(inputs[:, 0])
        anime_vector = self.anime_embedding(inputs[:, 1])
        anime_bias = self.anime_bias(inputs[:, 1])

        # Hitung dot product antara user dan anime embedding
        dot_user_anime = tf.tensordot(user_vector, anime_vector, axes=2)

        # Tambahkan bias
        x = dot_user_anime + user_bias + anime_bias
        return tf.nn.sigmoid(x)

# Normalisasi user_id dan anime_id ke dalam range embedding
from sklearn.preprocessing import LabelEncoder

# Encode user_id
user_encoder = LabelEncoder()
rate_df['user_id'] = user_encoder.fit_transform(rate_df['user_id'])

# Encode anime_id
anime_encoder = LabelEncoder()
rate_df['anime_id'] = anime_encoder.fit_transform(rate_df['anime_id'])

num_users = rate_df['user_id'].nunique()
num_anime = rate_df['anime_id'].nunique()

model = RecommenderNet(num_users,num_anime, 50) # inisialisasi model

# model compile
model.compile(
    loss = tf.keras.losses.BinaryCrossentropy(),
    optimizer = keras.optimizers.Adam(),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

# Data Preparation
x = rate_df[['user_id', 'anime_id']].values
y = rate_df['rating'].values

# Normalisasi rating ke skala 0-1
y = (y - y.min()) / (y.max() - y.min())

# Split data menjadi train dan test
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)

# Train Model
history = model.fit(
    x=x_train,
    y=y_train,
    batch_size=128,
    epochs=10,
    validation_data=(x_test, y_test)
)

# Visualisasi Hasil Training
plt.plot(history.history['root_mean_squared_error'], label='Train RMSE')
plt.plot(history.history['val_root_mean_squared_error'], label='Validation RMSE')
plt.title('Model Metrics')
plt.xlabel('Epochs')
plt.ylabel('RMSE')
plt.legend()
plt.show()

# Rekomendasi Anime
def recommend_anime(user_id, top_n=5):
    anime_watched_by_user = rate_df[rate_df.user_id == user_id]
    anime_not_watched = anime_df[~anime_df['anime_id'].isin(anime_watched_by_user['anime_id'])]
    anime_not_watched['user_id'] = user_id
    user_anime_array = anime_not_watched[['user_id', 'anime_id']].values

    # Prediksi rating untuk anime yang belum ditonton
    predictions = model.predict(user_anime_array)
    anime_not_watched['predicted_rating'] = predictions

    # Urutkan berdasarkan prediksi tertinggi
    recommendations = anime_not_watched.sort_values('predicted_rating', ascending=False).head(top_n)
    return recommendations[['name', 'genre', 'predicted_rating']]

user_id = 2  # Ganti dengan user_id lain
recommendations = recommend_anime(user_id)
print(recommendations)